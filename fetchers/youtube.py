import feedparser
import requests
import re
import os
from .base import BaseFetcher, FetchedItem


class YouTubeFetcher(BaseFetcher):
    """YouTube/æ’­å®¢æ•°æ®è·å–"""

    name = "youtube"
    name_zh = "YouTube/æ’­å®¢"
    icon = "ğŸ¬"
    color = "#ff0000"

    # é¢‘é“é…ç½®
    CHANNELS = {
        "UCSHZKyawb77ixDdsGog4iWA": "Lex Fridman",
        "UCbfYPyITQ-7l4upoX8nvctg": "Two Minute Papers",
        "UCNF5-lNi7Kqj2gYtGSz_GVQ": "AI Explained",
        "UCWN3xxRkmTPmbKwht9FuE5A": "Andrej Karpathy",
    }

    # æƒé‡é…ç½®
    CHANNEL_WEIGHTS = {
        "Lex Fridman": 100,
        "Andrej Karpathy": 95,
        "Two Minute Papers": 85,
        "AI Explained": 80,
    }

    ENTITY_WEIGHTS = {
        "openai": 50, "gpt-5": 50, "gpt-4": 40, "gpt": 30,
        "anthropic": 45, "claude": 45,
        "google": 40, "deepmind": 40, "gemini": 40,
        "nvidia": 35, "meta": 30,
        "sam altman": 50, "altman": 40,
        "andrej karpathy": 45, "karpathy": 40,
        "dario amodei": 40, "amodei": 35,
        "jensen huang": 35, "ilya sutskever": 45,
        "scaling": 25, "transformer": 25, "agent": 25, "sota": 30,
    }

    ENTITY_PATTERNS = [
        (r"openai", "OpenAI", "company"),
        (r"gpt-?[45o]", "GPT", "company"),
        (r"anthropic", "Anthropic", "company"),
        (r"claude", "Claude", "company"),
        (r"google|deepmind|gemini", "Google", "company"),
        (r"nvidia", "NVIDIA", "company"),
        (r"meta\s*ai|llama", "Meta AI", "company"),
        (r"sam\s*altman", "Sam Altman", "person"),
        (r"karpathy", "Karpathy", "person"),
        (r"dario", "Dario Amodei", "person"),
        (r"jensen", "Jensen Huang", "person"),
        (r"ilya", "Ilya Sutskever", "person"),
    ]

    MAX_DURATION_SECONDS = 30 * 60  # 30åˆ†é’Ÿ

    def __init__(self):
        super().__init__()
        self.rapidapi_key = os.getenv("RAPIDAPI_KEY", "")

    def fetch(self) -> list[FetchedItem]:
        """è·å–æ‰€æœ‰é¢‘é“çš„è§†é¢‘"""
        self.items = []

        for channel_id, channel_name in self.CHANNELS.items():
            print(f"  è·å–é¢‘é“: {channel_name}")
            entries = self._fetch_rss(channel_id)

            for entry in entries:
                pub_date = entry.get("published", "")
                if not self.is_within_hours(pub_date, 24):
                    continue

                video_id = entry.get("yt_videoid", "")
                if not video_id:
                    continue

                # è·å–æ—¶é•¿
                duration_seconds = self._get_duration(video_id)

                # è¿‡æ»¤è¶…é•¿è§†é¢‘
                if duration_seconds > self.MAX_DURATION_SECONDS:
                    print(f"    è·³è¿‡è¶…é•¿è§†é¢‘: {entry.get('title', '')[:30]}...")
                    continue

                item = FetchedItem(
                    id=video_id,
                    title=entry.get("title", ""),
                    link=entry.get("link", ""),
                    source=channel_name,
                    author=channel_name,
                    pub_date=pub_date,
                    thumbnail=f"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg",
                    extra={
                        "duration_seconds": duration_seconds,
                        "duration": self._format_duration(duration_seconds),
                        "thumbnail_mq": f"https://img.youtube.com/vi/{video_id}/mqdefault.jpg",
                    }
                )

                item.tags = self._extract_entities(item.title)
                item.fame_score = self._calculate_fame_score(item)

                self.items.append(item)
                print(f"    + {item.title[:40]}...")

        # æŒ‰çŸ¥ååº¦æ’åº
        self.items.sort(key=lambda x: x.fame_score, reverse=True)
        return self.items

    def _fetch_rss(self, channel_id: str) -> list:
        """è·å– YouTube é¢‘é“ RSS"""
        url = f"https://www.youtube.com/feeds/videos.xml?channel_id={channel_id}"
        try:
            feed = feedparser.parse(url)
            return feed.entries
        except Exception as e:
            print(f"    RSS è·å–å¤±è´¥: {e}")
            return []

    def _get_duration(self, video_id: str) -> int:
        """è·å–è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰"""
        if not self.rapidapi_key:
            return 0

        url = "https://youtube-media-downloader.p.rapidapi.com/v2/video/details"
        headers = {
            "X-RapidAPI-Key": self.rapidapi_key,
            "X-RapidAPI-Host": "youtube-media-downloader.p.rapidapi.com"
        }
        params = {"videoId": video_id}

        try:
            resp = requests.get(url, headers=headers, params=params, timeout=10)
            data = resp.json()
            return int(data.get("lengthSeconds", 0))
        except Exception as e:
            print(f"    è·å–æ—¶é•¿å¤±è´¥ {video_id}: {e}")
            return 0

    @staticmethod
    def _format_duration(seconds: int) -> str:
        """æ ¼å¼åŒ–æ—¶é•¿æ˜¾ç¤º"""
        if seconds == 0:
            return "N/A"
        mins = seconds // 60
        secs = seconds % 60
        return f"{mins}:{secs:02d}"

    def _extract_entities(self, title: str) -> list:
        """ä»æ ‡é¢˜æå–å…³é”®å®ä½“"""
        entities = []
        title_lower = title.lower()
        seen = set()

        for pattern, label, entity_type in self.ENTITY_PATTERNS:
            if re.search(pattern, title_lower) and label not in seen:
                entities.append({"label": label, "type": entity_type})
                seen.add(label)

        return entities

    def _calculate_fame_score(self, item: FetchedItem) -> int:
        """è®¡ç®—çŸ¥ååº¦åˆ†æ•°"""
        score = 0
        title_lower = item.title.lower()

        # é¢‘é“æƒé‡
        for name, weight in self.CHANNEL_WEIGHTS.items():
            if name.lower() in item.source.lower():
                score += weight
                break

        # å…³é”®è¯æƒé‡
        for keyword, weight in self.ENTITY_WEIGHTS.items():
            if keyword in title_lower:
                score += weight

        return score
